var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = GBModels","category":"page"},{"location":"#GBModels","page":"Home","title":"GBModels","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for GBModels.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [GBModels]","category":"page"},{"location":"#GBModels.LπDist","page":"Home","title":"GBModels.LπDist","text":"Laplace distribution with a point mass at 0.0\n\n\n\n\n\n","category":"type"},{"location":"#GBModels.NπDist","page":"Home","title":"GBModels.NπDist","text":"Gaussian distribution with a point mass at 0.0\n\n\n\n\n\n","category":"type"},{"location":"#GBModels.TπDist","page":"Home","title":"GBModels.TπDist","text":"T-distribution with a point mass at 0.0\n\n\n\n\n\n","category":"type"},{"location":"#Base.maximum-Tuple{GBModels.LπDist}","page":"Home","title":"Base.maximum","text":"Maximum value of the LπDist distribution\n\n\n\n\n\n","category":"method"},{"location":"#Base.maximum-Tuple{GBModels.NπDist}","page":"Home","title":"Base.maximum","text":"Maximum value of the NπDist distribution\n\n\n\n\n\n","category":"method"},{"location":"#Base.maximum-Tuple{GBModels.TπDist}","page":"Home","title":"Base.maximum","text":"Maximum value of the TπDist distribution\n\n\n\n\n\n","category":"method"},{"location":"#Base.minimum-Tuple{GBModels.LπDist}","page":"Home","title":"Base.minimum","text":"Minimum value of the LπDist distribution\n\n\n\n\n\n","category":"method"},{"location":"#Base.minimum-Tuple{GBModels.NπDist}","page":"Home","title":"Base.minimum","text":"Minimum value of the NπDist distribution\n\n\n\n\n\n","category":"method"},{"location":"#Base.minimum-Tuple{GBModels.TπDist}","page":"Home","title":"Base.minimum","text":"Minimum value of the TπDist distribution\n\n\n\n\n\n","category":"method"},{"location":"#Base.rand-Tuple{Random.AbstractRNG, GBModels.LπDist}","page":"Home","title":"Base.rand","text":"Sampling method for LπDist\n\nExamples\n\nd = LπDist(0.1, 0.0, 1.0)\nrand(d)\n\n\n\n\n\n","category":"method"},{"location":"#Base.rand-Tuple{Random.AbstractRNG, GBModels.NπDist}","page":"Home","title":"Base.rand","text":"Sampling method for NπDist\n\nExamples\n\nd = NπDist(0.1, 0.0, 1.0)\nrand(d)\n\n\n\n\n\n","category":"method"},{"location":"#Base.rand-Tuple{Random.AbstractRNG, GBModels.TπDist}","page":"Home","title":"Base.rand","text":"Sampling method for TπDist\n\nExamples\n\nd = TπDist(0.1, 1.0)\nrand(d)\n\n\n\n\n\n","category":"method"},{"location":"#Distributions.logpdf-Tuple{GBModels.LπDist, Real}","page":"Home","title":"Distributions.logpdf","text":"log(pdf) of LπDist\n\nExamples\n\nd = LπDist(0.1, 0.0, 1.0)\nlogpdf.(d, [-1.0, 0.0, 1.0])\n\n\n\n\n\n","category":"method"},{"location":"#Distributions.logpdf-Tuple{GBModels.NπDist, Real}","page":"Home","title":"Distributions.logpdf","text":"log(pdf) of NπDist\n\nExamples\n\nd = NπDist(0.1, 0.0, 1.0)\nlogpdf.(d, [-1.0, 0.0, 1.0])\n\n\n\n\n\n","category":"method"},{"location":"#Distributions.logpdf-Tuple{GBModels.TπDist, Real}","page":"Home","title":"Distributions.logpdf","text":"log(pdf) of TπDist\n\nExamples\n\nd = TπDist(0.1, 1.0)\nlogpdf.(d, [-1.0, 0.0, 1.0])\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.bayesian-Tuple{Function}","page":"Home","title":"GBModels.bayesian","text":"bayesian(\n    turing_model::Function;\n    X::Matrix{Float64},\n    y::Vector{Float64},\n    sampler::String = [\"NUTS\", \"HMC\", \"HMCDA\", \"MH\", \"PG\"][1],\n    sampling_method::Int64 = 1,\n    seed::Int64 = 123,\n    n_burnin::Int64 = 500,\n    n_iter::Int64 = 1_500,\n    verbose::Bool = false,\n)::Fit\n\nFit a Bayesian linear regression models via Turing.jl\n\nExamples\n\njulia> genomes = GBCore.simulategenomes(verbose=false);\n\njulia> trials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=1, f_add_dom_epi=[0.1 0.01 0.01;], verbose=false);;\n\njulia> phenomes = extractphenomes(trials);\n\njulia> sol = Suppressor.@suppress bayesian(turing_bayesG, genomes=genomes, phenomes=phenomes);\n\njulia> sol.metrics[\"cor\"] > 0.5\ntrue\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.bayesian-Tuple{String}","page":"Home","title":"GBModels.bayesian","text":"bayesian(\n    bglr_model::String;\n    X::Matrix{Float64},\n    y::Vector{Float64},\n    n_burnin::Int64 = 500,\n    n_iter::Int64 = 1_500,\n    verbose::Bool = false,\n)::Fit\n\nFit a Bayesian linear regression models via BGLR in R\n\nExamples\n\njulia> genomes = GBCore.simulategenomes(verbose=false);\n\njulia> trials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=1, f_add_dom_epi=[0.1 0.01 0.01;], verbose=false);;\n\njulia> phenomes = extractphenomes(trials);\n\njulia> sol = Suppressor.@suppress bayesian(\"BayesA\", genomes=genomes, phenomes=phenomes);\n\njulia> sol.metrics[\"cor\"] > 0.5\ntrue\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.bglr-Tuple{}","page":"Home","title":"GBModels.bglr","text":"Bayesian models using BGLR, i.e. Bayes A, Bayes B and Bayes C\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.extractxy-Tuple{}","page":"Home","title":"GBModels.extractxy","text":"extractxy(;\n    genomes::Genomes,\n    phenomes::Phenomes,\n    idx_entries::Vector{Int64},\n    idx_loci_alleles::Vector{Int64},\n    idx_trait::Int64 = 1,\n    add_intercept::Bool = true,\n)::Tuple{Matrix{Float64},Vector{Float64},Vector{String}}\n\nExtract explanatory X matrix, and response y vector from genomes and phenomes.\n\nExamples\n\njulia> genomes = GBCore.simulategenomes(verbose=false);\n\njulia> trials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=1, f_add_dom_epi=[0.1 0.01 0.01;], verbose=false);;\n\njulia> phenomes = extractphenomes(trials);\n\njulia> X, y, loci_alleles = extractxy(genomes=genomes, phenomes=phenomes);\n\njulia> X == hcat(ones(length(phenomes.entries)), genomes.allele_frequencies)\ntrue\n\njulia> y == phenomes.phenotypes[:, 1]\ntrue\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.lasso-Tuple{}","page":"Home","title":"GBModels.lasso","text":"lasso(; genomes::Genomes, phenomes::Phenomes, idx_trait::Int64 = 1, verbose::Bool = false)::Fit\n\nFit a LASSO (least absolute shrinkage and selection operator; L1) regression model\n\nExamples\n\njulia> genomes = GBCore.simulategenomes(verbose=false);\n\njulia> trials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=1, f_add_dom_epi=[0.1 0.01 0.01;], verbose=false);;\n\njulia> phenomes = extractphenomes(trials);\n\njulia> fit = lasso(genomes=genomes, phenomes=phenomes);\n\njulia> fit.model == \"lasso\"\ntrue\n\njulia> fit.metrics[\"cor\"] > 0.50\ntrue\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.logistic-Tuple{Function}","page":"Home","title":"GBModels.logistic","text":"logistic(\n    model::Function;\n    X::Matrix{Float64},\n    y::Vector{Float64},\n    verbose::Bool = false,\n)::Fit\n\nFit a linear model with binary response variable, using ols, ridge or lasso functions.\n\nExamples\n\njulia> genomes = GBCore.simulategenomes(verbose=false);\n\njulia> trials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=1, f_add_dom_epi=[0.1 0.01 0.01;], verbose=false);;\n\njulia> phenomes = extractphenomes(trials); y = phenomes.phenotypes[:, 1];\n\njulia> phenomes.phenotypes[:, 1] = (y .- minimum(y)) ./ (maximum(y) - minimum(y));\n\njulia> fit_ols = logistic(ols, genomes=genomes, phenomes=phenomes);\n\njulia> fit_ols.model == \"logistic-ols\"\ntrue\n\njulia> fit_ols.metrics[\"cor\"] > 0.50\ntrue\n\njulia> fit_ridge = logistic(ridge, genomes=genomes, phenomes=phenomes);\n\njulia> fit_ridge.model == \"logistic-ridge\"\ntrue\n\njulia> fit_ridge.metrics[\"cor\"] > 0.00\ntrue\n\njulia> fit_lasso = logistic(lasso, genomes=genomes, phenomes=phenomes);\n\njulia> fit_lasso.model == \"logistic-lasso\"\ntrue\n\njulia> fit_lasso.metrics[\"cor\"] > 0.00\ntrue\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.ols-Tuple{}","page":"Home","title":"GBModels.ols","text":"ols(; genomes::Genomes, phenomes::Phenomes, idx_trait::Int64 = 1, verbose::Bool = false)::Fit\n\nFit an ordinary least squares model\n\nExamples\n\njulia> genomes = GBCore.simulategenomes(verbose=false);\n\njulia> trials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=1, f_add_dom_epi=[0.1 0.01 0.01;], verbose=false);;\n\njulia> phenomes = extractphenomes(trials);\n\njulia> fit = ols(genomes=genomes, phenomes=phenomes);\n\njulia> fit.model == \"ols\"\ntrue\n\njulia> fit.metrics[\"cor\"] > 0.50\ntrue\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.ridge-Tuple{}","page":"Home","title":"GBModels.ridge","text":"ridge(; genomes::Genomes, phenomes::Phenomes, idx_trait::Int64 = 1, verbose::Bool = false)::Fit\n\nFit a ridge (L2) regression model\n\nExamples\n\njulia> genomes = GBCore.simulategenomes(verbose=false);\n\njulia> trials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=1, f_add_dom_epi=[0.1 0.01 0.01;], verbose=false);;\n\njulia> phenomes = extractphenomes(trials);\n\njulia> fit = ridge(genomes=genomes, phenomes=phenomes);\n\njulia> fit.model == \"ridge\"\ntrue\n\njulia> fit.metrics[\"cor\"] > 0.50\ntrue\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.turing_bayesG-Tuple{Any, Any}","page":"Home","title":"GBModels.turing_bayesG","text":"Turing specification of Bayesian linear regression using a Gaussian prior with common variance\n\nExample usage\n\n# Benchmarking\ngenomes = GBCore.simulategenomes(n=10, l=100)\ntrials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=3, f_add_dom_epi=[0.9 0.01 0.00;])\ntebv = GBCore.analyse(trials, max_levels = 15, max_time_per_model = 2)\nphenomes = tebv.phenomes[1]\nG::Matrix{Float64} = genomes.allele_frequencies\ny::Vector{Float64} = phenomes.phenotypes[:, 1]\nmodel = turing_bayesG(G, y)\nbenchmarks = TuringBenchmarking.benchmark_model(\n    model;\n    # Check correctness of computations\n    check=true,\n    # Automatic differentiation backends to check and benchmark\n    adbackends=[:forwarddiff, :reversediff, :reversediff_compiled, :zygote]\n)\n\n\n# Test more loci\ngenomes = GBCore.simulategenomes(n=10, l=10_000)\ntrials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=3, f_add_dom_epi=[0.9 0.01 0.00;])\ntebv = GBCore.analyse(trials, max_levels = 15, max_time_per_model = 2)\nphenomes = tebv.phenomes[1]\nG::Matrix{Float64} = genomes.allele_frequencies\ny::Vector{Float64} = phenomes.phenotypes[:, 1]\n# Check for uninferred types in the model\n@code_warntype model = turing_bayesG(G, y)\n# Fit\nmodel = turing_bayesG(G, y)\n# We use compile=true in AutoReverseDiff() because we do not have any if-statements in our Turing model below\nrng::TaskLocalRNG = Random.seed!(123)\nniter::Int64 = 1_500\nnburnin::Int64 = 500\n@time chain = Turing.sample(rng, model, NUTS(nburnin, 0.5, max_depth=5, Δ_max=1000.0, init_ϵ=0.2; adtype=AutoReverseDiff(compile=true)), niter-nburnin, progress=true);\n# Use the mean paramter values after 150 burn-in iterations\nparams = Turing.get_params(chain[150:end, :, :]);\nb_hat = vcat(mean(params.intercept), mean(stack(params.coefficients, dims=1)[:, :, 1], dims=2)[:,1]);\n# Assess prediction accuracy\ny_pred::Vector{Float64} = hcat(ones(size(G,1)), G) * b_hat;\nUnicodePlots.scatterplot(y, y_pred)\nperformance::Dict{String, Float64} = metrics(y, y_pred)\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.turing_bayesGs-Tuple{Any, Any}","page":"Home","title":"GBModels.turing_bayesGs","text":"Turing specification of Bayesian linear regression using a Gaussian prior with varying variances\n\nExample usage\n\n# Simulate data\ngenomes = GBCore.simulategenomes(n=10, l=100)\ntrials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=3, f_add_dom_epi=[0.9 0.01 0.00;])\ntebv = GBCore.analyse(trials, max_levels = 15, max_time_per_model = 2)\nphenomes = tebv.phenomes[1]\n# Extract genotype and phenotype data\nG::Matrix{Float64} = genomes.allele_frequencies\ny::Vector{Float64} = phenomes.phenotypes[:, 1]\n# Regress for just 200 iterations for demonstration purposes only. Use way way more iterations, e.g. 10,000.\nmodel = turing_bayesGs(G, y)\n# We use compile=true in AutoReverseDiff() because we do not have any if-statements in our Turing model below\nrng::TaskLocalRNG = Random.seed!(123)\nniter::Int64 = 1_500\nnburnin::Int64 = 500\n@time chain = Turing.sample(rng, model, NUTS(nburnin, 0.5, max_depth=5, Δ_max=1000.0, init_ϵ=0.2; adtype=AutoReverseDiff(compile=true)), niter-nburnin, progress=true);\n# Use the mean paramter values after 150 burn-in iterations\nparams = Turing.get_params(chain[150:end, :, :]);\nb_hat = vcat(mean(params.intercept), mean(stack(params.coefficients, dims=1)[:, :, 1], dims=2)[:,1]);\n# Assess prediction accuracy\ny_pred::Vector{Float64} = hcat(ones(size(G,1)), G) * b_hat;\nUnicodePlots.scatterplot(y, y_pred)\nperformance::Dict{String, Float64} = metrics(y, y_pred)\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.turing_bayesGπ-Tuple{Any, Any}","page":"Home","title":"GBModels.turing_bayesGπ","text":"Turing specification of Bayesian linear regression using a Gaussian prior with a point mass at zero and common variance\n\nExample usage\n\n# Simulate data\ngenomes = GBCore.simulategenomes(n=10, l=100)\ntrials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=3, f_add_dom_epi=[0.9 0.01 0.00;])\ntebv = GBCore.analyse(trials, max_levels = 15, max_time_per_model = 2)\nphenomes = tebv.phenomes[1]\n# Extract genotype and phenotype data\nG::Matrix{Float64} = genomes.allele_frequencies\ny::Vector{Float64} = phenomes.phenotypes[:, 1]\n# Regress for just 200 iterations for demonstration purposes only. Use way way more iterations, e.g. 10,000.\nmodel = turing_bayesGπ(G, y)\n# We use compile=true in AutoReverseDiff() because we do not have any if-statements in our Turing model below\nrng::TaskLocalRNG = Random.seed!(123)\nniter::Int64 = 1_500\nnburnin::Int64 = 500\n@time chain = Turing.sample(rng, model, NUTS(nburnin, 0.5, max_depth=5, Δ_max=1000.0, init_ϵ=0.2; adtype=AutoReverseDiff(compile=true)), niter-nburnin, progress=true);\n# Use the mean paramter values after 150 burn-in iterations\nparams = Turing.get_params(chain[150:end, :, :]);\nb_hat = vcat(mean(params.intercept), mean(stack(params.coefficients, dims=1)[:, :, 1], dims=2)[:,1]);\n# Assess prediction accuracy\ny_pred::Vector{Float64} = hcat(ones(size(G,1)), G) * b_hat;\nUnicodePlots.scatterplot(y, y_pred)\nperformance::Dict{String, Float64} = metrics(y, y_pred)\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.turing_bayesGπs-Union{Tuple{T}, Tuple{Any, Any}, Tuple{Any, Any, DynamicPPL.TypeWrap{T}}} where T","page":"Home","title":"GBModels.turing_bayesGπs","text":"Turing specification of Bayesian linear regression using a Gaussian prior with a point mass at zero and varying variances\n\nExample usage\n\n# Simulate data\ngenomes = GBCore.simulategenomes(n=10, l=100)\ntrials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=3, f_add_dom_epi=[0.9 0.01 0.00;])\ntebv = GBCore.analyse(trials, max_levels = 15, max_time_per_model = 2)\nphenomes = tebv.phenomes[1]\n# Extract genotype and phenotype data\nG::Matrix{Float64} = genomes.allele_frequencies\ny::Vector{Float64} = phenomes.phenotypes[:, 1]\n# Regress for just 200 iterations for demonstration purposes only. Use way way more iterations, e.g. 10,000.\nmodel = turing_bayesGπs(G, y)\n# We use compile=true in AutoReverseDiff() because we do not have any if-statements in our Turing model below\nrng::TaskLocalRNG = Random.seed!(123)\nniter::Int64 = 1_500\nnburnin::Int64 = 500\n@time chain = Turing.sample(rng, model, NUTS(nburnin, 0.5, max_depth=5, Δ_max=1000.0, init_ϵ=0.2; adtype=AutoReverseDiff(compile=true)), niter-nburnin, progress=true);\n# Use the mean paramter values after 150 burn-in iterations\nparams = Turing.get_params(chain[150:end, :, :]);\nb_hat = vcat(mean(params.intercept), mean(stack(params.coefficients, dims=1)[:, :, 1], dims=2)[:,1]);\n# Assess prediction accuracy\ny_pred::Vector{Float64} = hcat(ones(size(G,1)), G) * b_hat;\nUnicodePlots.scatterplot(y, y_pred)\nperformance::Dict{String, Float64} = metrics(y, y_pred)\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.turing_bayesL-Tuple{Any, Any}","page":"Home","title":"GBModels.turing_bayesL","text":"Turing specification of Bayesian linear regression using a Laplacian prior with a common scale\n\nExample usage\n\n# Simulate data\ngenomes = GBCore.simulategenomes(n=10, l=100)\ntrials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=3, f_add_dom_epi=[0.9 0.01 0.00;])\ntebv = GBCore.analyse(trials, max_levels = 15, max_time_per_model = 2)\nphenomes = tebv.phenomes[1]\n# Extract genotype and phenotype data\nG::Matrix{Float64} = genomes.allele_frequencies\ny::Vector{Float64} = phenomes.phenotypes[:, 1]\n# Regress for just 200 iterations for demonstration purposes only. Use way way more iterations, e.g. 10,000.\nmodel = turing_bayesL(G, y)\n# We use compile=true in AutoReverseDiff() because we do not have any if-statements in our Turing model below\nrng::TaskLocalRNG = Random.seed!(123)\nniter::Int64 = 1_500\nnburnin::Int64 = 500\n@time chain = Turing.sample(rng, model, NUTS(nburnin, 0.5, max_depth=5, Δ_max=1000.0, init_ϵ=0.2; adtype=AutoReverseDiff(compile=true)), niter-nburnin, progress=true);\n# Use the mean paramter values after 150 burn-in iterations\nparams = Turing.get_params(chain[150:end, :, :]);\nb_hat = vcat(mean(params.intercept), mean(stack(params.coefficients, dims=1)[:, :, 1], dims=2)[:,1]);\n# Assess prediction accuracy\ny_pred::Vector{Float64} = hcat(ones(size(G,1)), G) * b_hat;\nUnicodePlots.scatterplot(y, y_pred)\nperformance::Dict{String, Float64} = metrics(y, y_pred)\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.turing_bayesLs-Union{Tuple{T}, Tuple{Any, Any}, Tuple{Any, Any, DynamicPPL.TypeWrap{T}}} where T","page":"Home","title":"GBModels.turing_bayesLs","text":"Turing specification of Bayesian linear regression using a Laplacian prior with varying scales\n\nExample usage\n\n# Simulate data\ngenomes = GBCore.simulategenomes(n=10, l=100)\ntrials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=3, f_add_dom_epi=[0.9 0.01 0.00;])\ntebv = GBCore.analyse(trials, max_levels = 15, max_time_per_model = 2)\nphenomes = tebv.phenomes[1]\n# Extract genotype and phenotype data\nG::Matrix{Float64} = genomes.allele_frequencies\ny::Vector{Float64} = phenomes.phenotypes[:, 1]\n# Regress for just 200 iterations for demonstration purposes only. Use way way more iterations, e.g. 10,000.\nmodel = turing_bayesLs(G, y)\n# We use compile=true in AutoReverseDiff() because we do not have any if-statements in our Turing model below\nrng::TaskLocalRNG = Random.seed!(123)\nniter::Int64 = 1_500\nnburnin::Int64 = 500\n@time chain = Turing.sample(rng, model, NUTS(nburnin, 0.5, max_depth=5, Δ_max=1000.0, init_ϵ=0.2; adtype=AutoReverseDiff(compile=true)), niter-nburnin, progress=true);\n# Use the mean paramter values after 150 burn-in iterations\nparams = Turing.get_params(chain[150:end, :, :]);\nb_hat = vcat(mean(params.intercept), mean(stack(params.coefficients, dims=1)[:, :, 1], dims=2)[:,1]);\n# Assess prediction accuracy\ny_pred::Vector{Float64} = hcat(ones(size(G,1)), G) * b_hat;\nUnicodePlots.scatterplot(y, y_pred)\nperformance::Dict{String, Float64} = metrics(y, y_pred)\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.turing_bayesLπ-Tuple{Any, Any}","page":"Home","title":"GBModels.turing_bayesLπ","text":"Turing specification of Bayesian linear regression using a Laplacian prior with a point mass at zero and common scale\n\nExample usage\n\n# Simulate data\ngenomes = GBCore.simulategenomes(n=10, l=100)\ntrials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=3, f_add_dom_epi=[0.9 0.01 0.00;])\ntebv = GBCore.analyse(trials, max_levels = 15, max_time_per_model = 2)\nphenomes = tebv.phenomes[1]\n# Extract genotype and phenotype data\nG::Matrix{Float64} = genomes.allele_frequencies\ny::Vector{Float64} = phenomes.phenotypes[:, 1]\n# Regress for just 200 iterations for demonstration purposes only. Use way way more iterations, e.g. 10,000.\nmodel = turing_bayesLπ(G, y)\n# We use compile=true in AutoReverseDiff() because we do not have any if-statements in our Turing model below\nrng::TaskLocalRNG = Random.seed!(123)\nniter::Int64 = 1_500\nnburnin::Int64 = 500\n@time chain = Turing.sample(rng, model, NUTS(nburnin, 0.5, max_depth=5, Δ_max=1000.0, init_ϵ=0.2; adtype=AutoReverseDiff(compile=true)), niter-nburnin, progress=true);\n# Use the mean paramter values after 150 burn-in iterations\nparams = Turing.get_params(chain[150:end, :, :]);\nb_hat = vcat(mean(params.intercept), mean(stack(params.coefficients, dims=1)[:, :, 1], dims=2)[:,1]);\n# Assess prediction accuracy\ny_pred::Vector{Float64} = hcat(ones(size(G,1)), G) * b_hat;\nUnicodePlots.scatterplot(y, y_pred)\nperformance::Dict{String, Float64} = metrics(y, y_pred)\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.turing_bayesLπs-Union{Tuple{T}, Tuple{Any, Any}, Tuple{Any, Any, DynamicPPL.TypeWrap{T}}} where T","page":"Home","title":"GBModels.turing_bayesLπs","text":"Turing specification of Bayesian linear regression using a Laplacian prior with a point mass at zero and common scale\n\nExample usage\n\n# Simulate data\ngenomes = GBCore.simulategenomes(n=10, l=100)\ntrials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=3, f_add_dom_epi=[0.9 0.01 0.00;])\ntebv = GBCore.analyse(trials, max_levels = 15, max_time_per_model = 2)\nphenomes = tebv.phenomes[1]\n# Extract genotype and phenotype data\nG::Matrix{Float64} = genomes.allele_frequencies\ny::Vector{Float64} = phenomes.phenotypes[:, 1]\n# Regress for just 200 iterations for demonstration purposes only. Use way way more iterations, e.g. 10,000.\nmodel = turing_bayesLπs(G, y)\n# We use compile=true in AutoReverseDiff() because we do not have any if-statements in our Turing model below\nrng::TaskLocalRNG = Random.seed!(123)\nniter::Int64 = 1_500\nnburnin::Int64 = 500\n@time chain = Turing.sample(rng, model, NUTS(nburnin, 0.5, max_depth=5, Δ_max=1000.0, init_ϵ=0.2; adtype=AutoReverseDiff(compile=true)), niter-nburnin, progress=true);\n# Use the mean paramter values after 150 burn-in iterations\nparams = Turing.get_params(chain[150:end, :, :]);\nb_hat = vcat(mean(params.intercept), mean(stack(params.coefficients, dims=1)[:, :, 1], dims=2)[:,1]);\n# Assess prediction accuracy\ny_pred::Vector{Float64} = hcat(ones(size(G,1)), G) * b_hat;\nUnicodePlots.scatterplot(y, y_pred)\nperformance::Dict{String, Float64} = metrics(y, y_pred)\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.turing_bayesT-Tuple{Any, Any}","page":"Home","title":"GBModels.turing_bayesT","text":"Turing specification of Bayesian linear regression using a T-distribution\n\nExample usage\n\n# Simulate data\ngenomes = GBCore.simulategenomes(n=10, l=100)\ntrials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=3, f_add_dom_epi=[0.9 0.01 0.00;])\ntebv = GBCore.analyse(trials, max_levels = 15, max_time_per_model = 2)\nphenomes = tebv.phenomes[1]\n# Extract genotype and phenotype data\nG::Matrix{Float64} = genomes.allele_frequencies\ny::Vector{Float64} = phenomes.phenotypes[:, 1]\n# Regress for just 200 iterations for demonstration purposes only. Use way way more iterations, e.g. 10,000.\nmodel = turing_bayesT(G, y)\n# We use compile=true in AutoReverseDiff() because we do not have any if-statements in our Turing model below\nrng::TaskLocalRNG = Random.seed!(123)\nniter::Int64 = 1_500\nnburnin::Int64 = 500\n@time chain = Turing.sample(rng, model, NUTS(nburnin, 0.5, max_depth=5, Δ_max=1000.0, init_ϵ=0.2; adtype=AutoReverseDiff(compile=true)), niter-nburnin, progress=true);\n# Use the mean paramter values after 150 burn-in iterations\nparams = Turing.get_params(chain[150:end, :, :]);\nb_hat = vcat(mean(params.intercept), mean(stack(params.coefficients, dims=1)[:, :, 1], dims=2)[:,1]);\n# Assess prediction accuracy\ny_pred::Vector{Float64} = hcat(ones(size(G,1)), G) * b_hat;\nUnicodePlots.scatterplot(y, y_pred)\nperformance::Dict{String, Float64} = metrics(y, y_pred)\n\n\n\n\n\n","category":"method"},{"location":"#GBModels.turing_bayesTπ-Tuple{Any, Any}","page":"Home","title":"GBModels.turing_bayesTπ","text":"Turing specification of Bayesian linear regression using a T-distribution with a point mass at zero\n\nExample usage\n\n# Simulate data\ngenomes = GBCore.simulategenomes(n=10, l=100)\ntrials, _ = GBCore.simulatetrials(genomes=genomes, n_years=1, n_seasons=1, n_harvests=1, n_sites=1, n_replications=3, f_add_dom_epi=[0.9 0.01 0.00;])\ntebv = GBCore.analyse(trials, max_levels = 15, max_time_per_model = 2)\nphenomes = tebv.phenomes[1]\n# Extract genotype and phenotype data\nG::Matrix{Float64} = genomes.allele_frequencies\ny::Vector{Float64} = phenomes.phenotypes[:, 1]\n# Regress for just 200 iterations for demonstration purposes only. Use way way more iterations, e.g. 10,000.\nmodel = turing_bayesTπ(G, y)\n# We use compile=true in AutoReverseDiff() because we do not have any if-statements in our Turing model below\nrng::TaskLocalRNG = Random.seed!(123)\nniter::Int64 = 1_500\nnburnin::Int64 = 500\n@time chain = Turing.sample(rng, model, NUTS(nburnin, 0.5, max_depth=5, Δ_max=1000.0, init_ϵ=0.2; adtype=AutoReverseDiff(compile=true)), niter-nburnin, progress=true);\n# Use the mean paramter values after 150 burn-in iterations\nparams = Turing.get_params(chain[150:end, :, :]);\nb_hat = vcat(mean(params.intercept), mean(stack(params.coefficients, dims=1)[:, :, 1], dims=2)[:,1]);\n# Assess prediction accuracy\ny_pred::Vector{Float64} = hcat(ones(size(G,1)), G) * b_hat;\nUnicodePlots.scatterplot(y, y_pred)\nperformance::Dict{String, Float64} = metrics(y, y_pred)\n\n\n\n\n\n","category":"method"}]
}
